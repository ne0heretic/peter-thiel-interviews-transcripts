[A.I., Mars and Immortality: Are We Dreaming Big Enough? | Interesting Times with Ross Douthat](https://youtu.be/vV7YgnPUxcU)

[00:00:00] Is Silicon Valley recklessly ambitious?

[00:00:03] What should we fear more:Armageddon or stagnation?

[00:00:10] Why is one of the world’s most successful investors worrying

[00:00:13] about the Antichrist?My guest today is the co-founder

[00:00:18] of PayPal and Palantir, and an early investor

[00:00:22] in the political careers of Donald Trump and JD Vance.

[00:00:26] Peter Thiel is the original tech right power player,

[00:00:30] well known for funding a range of conservative and simply

[00:00:34] contrarian ideas.But we’re going to talk about his own ideas because despite

[00:00:39] the slight handicap of being a billionaire,there’s a good case that he’s the most influential right

[00:00:45] wing intellectual of the last 20 years.

[00:00:54] Peter Thiel, welcome to "Interesting Times."

[00:00:57] Thanks for having me.You’re very welcome.Thanks for being here.So I want to start by taking you back in time,

[00:01:04] about 13 or 14 years.You wrote an essay for National Review.

[00:01:08] The conservative magazine called "The End of the Future."

[00:01:12] And basically the argument in that essay

[00:01:16] was that the dynamic, fast-paced, ever-changing

[00:01:20] modern world was just not nearly as dynamicas people thought,

[00:01:24] and that actually, we'd entered a periodof technological stagnation.

[00:01:29] That digital life was a breakthrough, but not

[00:01:33] as big a breakthrough as people had hoped.And that the world was kind of stuck, basically.

[00:01:40] And you weren’t the only person to make arguments likethis, but it had a special potency coming from you

[00:01:46] because you were a Silicon Valley insider who had gotten

[00:01:50] rich in the digital revolution.So I’m curious: In 2025, right.

[00:01:56] Do you think that diagnosis still holds?Yes, I still broadly believe in the stagnation thesis.

[00:02:06] It was never an absolute thesis.So the claim was not that we were absolutely, completely

[00:02:11] stuck.It was in some ways a claim about the velocity had slowed.

[00:02:17] It wasn’t 0, but that we were, I don’t know. From 1750 to 1970,

[00:02:24] 200 plus years, were periods of accelerating change where we

[00:02:28] were, relentlessly.We’re moving faster.The ships were faster, the railroads were faster,

[00:02:34] the cars were faster, the planes were faster.It culminates in the Concorde and the Apollo missions.

[00:02:39] And then that in all sorts of dimensions, things had slowed.

[00:02:47] There was, I always made an exceptionfor the world of bits.

[00:02:51] So we had computers and software and internetand mobile internet.

[00:02:56] And then the last 10, 15 years you had crypto and the A.I.

[00:03:00] revolution, which I think is in some sense pretty big.

[00:03:04] But the question is: Is it enough to really get out

[00:03:10] of this generalized sense of stagnation?And there’s an epistemological question you can start with

[00:03:16] on the "Back to the Future" essays:How do we even how do we even know whether we’re

[00:03:22] in stagnation or acceleration?Because one of the features of late modernity

[00:03:26] is that people are hyperspecialized.And so, can you say that we’re not making progress in physics

[00:03:34] unless you’ve devoted half your life to studying stringtheory?Or what about quantum computers?

[00:03:39] Or what about cancer research and biotech and all

[00:03:45] these verticals?And then how much does progress

[00:03:50] in cancer count versus string theory?So you have to give weightings to all these things.

[00:03:55] So it’s, in theory it’s an extremely,an extremely difficult question to get a handle

[00:04:01] of because - yeah, the fact that it’s so hard to answer that wehave ever narrower groups of guardians guarding themselves

[00:04:07] is itself cause for skepticism.And so yes, I think broadly we’re in this world that’s

[00:04:16] still pretty stuck.It’s not absolutely stuck.Yeah. You mentioned "Back to the Future."

[00:04:20] We just showed our kids the original "Back

[00:04:24] to the Future."The first one with Michael J. Fox and of course-Yeah, it was like 1955 to 1985, 30 years back.

[00:04:30] And then the "Back to Future II"was, I think 1985 to 2015, which

[00:04:35] is now a decade in the past.And that’s where you had flying cars.And the 2015 future is wildly divergent from the 1985.

[00:04:43] The 2015 future.did have Biff Tannen as a Donald Trump-like

[00:04:47] figure in some kind of power.So it had some kind of prescience.

[00:04:52] But yeah, the big, the big noticeable thing

[00:04:56] is just how different the built environment looks.And so one of the strongest cases for stagnation that I’ve

[00:05:03] heard is that yeah, if you put someone in a time machine from

[00:05:07] various points, they would recognize themselves to bein a completely different world if they left 1860 or 18

[00:05:14] 90 to 1970, if those were the 80 years of your lifetimeor something like that.

[00:05:18] But the world just to my kids, even as children of 2025,

[00:05:22] looking at 1985, it’s like the cars were a little different.And no one has phones, but the world seems fairly similar.

[00:05:31] So that’s a kind of non-statistical.But that’s the common sense.

[00:05:36] That’s the common sense understanding.But are there what would convince you

[00:05:40] that we were living through a period of takeoff.Is it just economic growth.

[00:05:44] Is it productivity growth.Like what are their numbers for stagnation versus dynamism

[00:05:50] that you look at.Sure it would be.Well, the economic number would just

[00:05:54] be what are your living standards comparedto your parents.

[00:05:58] If you’re a 30-year-old millennial or how are you

[00:06:05] doing versus when your parents,your Boomer parents were 30 years old,

[00:06:08] how did they do at that time.There are intellectual questions.

[00:06:13] How many breakthroughs are we having.

[00:06:17] How do we quantify these things.What are the returns of going into research.

[00:06:22] There certainly are diminishing returnsto going into science or going into academia generally.

[00:06:27] And then maybe this is why I’m so much of it feels like this

[00:06:31] sociopathic, Malthusian kind of an institution,

[00:06:35] because you have to throw more and more and more at somethingto get the same returns.

[00:06:39] And at some point, people give up and the thing collapses.Well, right.

[00:06:43] So let’s pick up on that.Why should we want growth and dynamism because, as you’ve

[00:06:50] pointed out in some of your arguments on the subject,right, there is a kind of cultural change that happens

[00:06:55] in the Western world in the 1970s,around the time you think things slow down and start

[00:07:01] to stagnate, where people become very anxious aboutthe costs of growth, the environmental costs,

[00:07:06] above all.And the idea being end up with a widely shared perspective

[00:07:13] that we’re rich enough.And if we try too hard to get that much richer,

[00:07:19] the planet won’t be able to support us.We’ll have degradation of various kinds.

[00:07:24] And we should be content with where we are.So what’s wrong with that argument.

[00:07:30] Well, I think there are deep reasonsthe stagnation happened.

[00:07:34] So there are always three questionsyou can ask about history.What actually happened.And there’s a question you get to what should be done about

[00:07:39] it.But there’s also this intermediate question why didit happen.

[00:07:43] People ran out of ideas, I thinkto some extent the institutions degraded

[00:07:49] and became risk averse.And these cultural transformationswe can describe.

[00:07:54] But then I think to some extent,also people had some very legitimate worries

[00:07:59] about the future, where if we continueto have accelerating progress, were you accelerating

[00:08:04] towards environmental apocalypseor nuclear apocalypse or things like that.

[00:08:10] But I think if we don’t find a way back to the future,

[00:08:14] I do think the society.I don’t it unravels.

[00:08:18] It doesn’t work.The way middle the middle class.

[00:08:23] I would define the middle classas the people who expect their kids to do better

[00:08:27] than themselves.And when that expectation collapses,

[00:08:31] we no longer have a middle class society.And maybe there’s I mean, maybe there’s some way you can

[00:08:36] have a feudal society in which things are always staticand stuck, or maybe there’s some way you can shift to some

[00:08:43] radically different society, but it’s not the way.It’s not the way the Western world.

[00:08:47] It’s not the way the United States has functionedfor the first 200 years of its existence.

[00:08:53] So you think that ordinary people won’t accept stagnation

[00:08:58] in the end, it’s that they will rebel and pull things

[00:09:03] down around them in the course of that rebellion.They may rebel or our institutions don’t work.

[00:09:10] All of our institutions are predicated on growth.Our budget, our budgets are certainly

[00:09:15] predicated on growth.Yeah if you say, I don’t Reagan and Obama Reagan was

[00:09:22] consumer capitalism, which is oxymoronic.

[00:09:26] It was borrow you don’t save money as a capitalist.

[00:09:30] You borrow money.And Obama was low tax socialism,

[00:09:33] just as oxymoronic as the consumerist capitalism

[00:09:39] of Reagan.And yeah, I low tech socialism way better than high tax

[00:09:45] socialism, but I worry that it’s not sustainable.

[00:09:49] At some point you either the taxes go upor the socialism ends.

[00:09:54] So it’s deeply, deeply unstable.

[00:09:58] And that’s why people are they’re not optimistic.

[00:10:02] They don’t think we’ve hit some stable the Greta future.

[00:10:07] Maybe it can work.This is the Greta Thunberg.Just to be clear, that’s a reference to Greta Thunberg,

[00:10:13] the activist best known for anti-climate change protests.

[00:10:18] Who to you, I would say representsa kind of symbol of a anti-growth, effectively

[00:10:25] authoritarian, environmentalistdominated future.

[00:10:29] Sure but we’re not there yet.We’re not there yet.It would be.

[00:10:33] It would be a very, very different societyif you actually lived in a kind

[00:10:37] of degrowth small Scandinavian villages.

[00:10:41] I’m not sure would be North Korea, but it would be.It would be super oppressive.

[00:10:45] One thing that’s always struck me is that when you have this

[00:10:52] sense of stagnation, a sense of decadence,

[00:10:56] right to use a word that I like to use for itin a society.

[00:11:01] You then also have people who end upbeing kind of eager for a crisis, right.

[00:11:06] Eager for a moment to come along where they can.

[00:11:10] They can radically redirect society from the path it’s on.Because I tend to think that in rich societies

[00:11:16] you hit a certain level of wealth.People become very comfortable,they become risk averse, and it’s just hard.

[00:11:22] It’s hard to get out of decadence, into something,into something new, without a crisis.

[00:11:27] So the original example for me was after September 11.There was this whole mentality among foreign policy

[00:11:33] conservatives that we had been decadent and stagnant,and now is our time to wake up and launch a new crusade

[00:11:41] and remake the world.And obviously that ended very badly.But something similar it was Bush 43

[00:11:46] just told people to go shopping right away.So it wasn’t anti decadent for the most part.

[00:11:52] So there was maybe there was some neocon foreign policy

[00:11:57] enclave in which people were LARPing as a wayto get out of decadence.

[00:12:01] But the dominant thing was Bush 43 people telling people

[00:12:05] just to go shopping.So what risks should you be willing to take

[00:12:08] to escape decadence?It does seem like there’s a danger here where the people

[00:12:17] who want to be anti decadent have to take on a lot of risk.

[00:12:21] They have to say, look, you’ve got this nice, stable,comfortable society.

[00:12:25] But guess what.We’d we’d like to have a war or a crisis or a total

[00:12:31] reorganization of government and so on.They have to lean into danger.

[00:12:36] I don’t know if I have to answer.I don’t know if I have to give you a precise answer,

[00:12:41] but my directional answer is a lot more.We should take a lot more risk.

[00:12:45] We should be doing a lot more.And I can go through all these different verticals.

[00:12:52] If we look at biotech, something like dementia,

[00:12:56] Alzheimer’s, we’ve made zero progress in 40 to 50 years.

[00:13:01] People are completely stuck on beta amyloid.It’s obviously not working.

[00:13:06] It’s just some kind of a stupid racket where the people

[00:13:10] are just reinforcing themselves.And so Yes, we need to take way

[00:13:15] more risk in that department.Well, let’s I want to ask to keep us in the concrete.

[00:13:20] I want to stay with that example for a minuteand ask, O.K, what does that mean.

[00:13:25] Saying we need to take more risks in anti-aging research.

[00:13:30] Does it mean that the FDA has to step back and say,

[00:13:34] anyone who has a new treatment for Alzheimer’s can go ahead

[00:13:38] and sell it on the open market.Like what is risk in the medical space look like.

[00:13:45] Yeah you would take you would take a lot more risk.If you have disease, there probably are a lot more risks.

[00:13:53] You can take.There are a lot more risks the researchers can take.

[00:13:57] Culturally, what I imagine it looks like is early modernitywhere people Yeah, they thought

[00:14:03] we would cure diseases.They thought we would have radical life extension.

[00:14:07] Immortality that was part of the projectof early modernity.

[00:14:11] It was Francis Bacon, Condorcet it wasand maybe it was maybe it was anti-Christian,

[00:14:17] maybe it was downstream of Christianity.It was competitive.If Christianity promised you a physical Resurrection science

[00:14:26] was not going to succeed unless it promised youthe exact same thing.But I remember 1999, 99 2000.

[00:14:33] When we were running PayPal, one of my co-founders,Luke Nozick, he was into Alcor and cryonics and people

[00:14:40] should freeze themselves and and we had one day

[00:14:44] where we took the whole company to a freezing party,a Tupperware party.

[00:14:47] People sell Tupperware policies at a freezing party.They sell their.Was it just your heads.

[00:14:52] What was going to be frozen.You could get could get a full body or just a head.

[00:14:57] Just the head was cheaper.It was disturbing.With a dot matrix printer didn’t quite work.

[00:15:01] And so the freezing policies couldn’t be couldn’t beprinted out.But in retrospect, this was still technological stagnation

[00:15:09] once again.But it was.But it’s also a symptom of the decline because in 1999,

[00:15:15] this was not a mainstream view,but there was still a fringe Boomer view where they still

[00:15:20] believed they could live forever.And that was the last generation.

[00:15:24] And so I’m always anti Boomer, but maybe there’s somethingwe’ve lost even in this fringe Boomer narcissism where there

[00:15:31] were at least a few boomers who still believed science

[00:15:35] would cure all their diseases.No one, no one who’s a millennial believes that

[00:15:40] anymore.I think there are some people who

[00:15:44] believe in a different kind of immortality, though,right now.

[00:15:49] I think part of the fascination with A.I.

[00:15:54] is connected to a specific, a specific visionof transcending limits.

[00:16:01] And I’m going to ask you about that after I ask you about politics, because one

[00:16:05] of the striking things I thought about your originalargument on stagnation, which was mostly about technology

[00:16:11] and the economy, was that it could be applied to a prettywide range of things.

[00:16:15] And at the time you were writing that essay,you were interested in seasteading.

[00:16:20] This idea of ideas of essentially buildingnew polities independent of the sclerotic Western world.

[00:16:28] But then you made a pivot in the 2010s.So you were one of the few prominent, maybe

[00:16:34] the only prominent Silicon Valleysupporter of Donald Trump.In 2016, you supported a few carefully selected

[00:16:43] Republican Senate candidates.One of them is now the vice president

[00:16:47] of the United States.And my view as an observer of what you were doing

[00:16:53] was that you were basically being a kind of venturecapitalist for politics.

[00:16:58] You were saying, here are some disruptive agents who might

[00:17:02] change the political status quo,and it’s worth a certain kind of risk here.

[00:17:08] Is that how you thought about it.Sure there were all sorts of levels.I mean, one level was yeah, it was these hopes that we could

[00:17:15] redirect the Titanic from the iceberg it was heading toor whatever the metaphor has really

[00:17:21] changed course as a society through political,

[00:17:24] through political change.Maybe a narrower a much narrower aspiration

[00:17:28] was that we could maybe at leasthave a conversation about this when someone like Trump

[00:17:34] said, make America great again.O.K is that a positive, optimistic, ambitious agenda,

[00:17:42] or is it merely a very pessimistic assessment

[00:17:49] of where that we are no longer a great country.And I didn’t have great expectations about what Trump

[00:17:56] would do in a positive way.But I thought, at least for the first time in 100 years,

[00:18:02] we had a Republican who was not giving us this syrupy Bush

[00:18:08] nonsense.And that was not the same as progress,

[00:18:13] but we could at least have a conversation.In retrospect, this was a preposterous fantasy.

[00:18:18] I had these two thoughts and in 2016,

[00:18:22] and you often have these ideas thatare just below the level of your consciousness.

[00:18:27] But the two thoughts I had that I wasn’t able to combinewas, number one, no, nobody would be mad at me

[00:18:34] for supporting Trump if he lost.And number two, I thought he had a 50/50 chance of winning.

[00:18:40] And then I had this implicit, why would nobody be mad at you

[00:18:45] if he lost.It would just be such a weird thing and it wouldn’t reallymatter.But then I thought he had more.

[00:18:52] He had.I thought he had a 50/50 chance because the problemswere deep and the stagnation was frustrating.

[00:18:58] And then the fantasy I had was yeah, if he won,

[00:19:02] we could have this conversation.And the reality was people weren’t ready for it.

[00:19:06] And then maybe we’ve progressed to the point wherewe can have this conversation at this point in 2025,

[00:19:14] a decade after Trump.And of course, you’re not a zombie left wing person.

[00:19:20] Ross but but this is I’ve been called many things.

[00:19:24] Many things I’ll take.I’ll take whatever progress I can get.So from your perspective of.

[00:19:29] So let’s say there’s two layers.There’s a basic sense of this society needs disruption.

[00:19:36] It needs risk.Trump is disruption, Trump is risk and Trump is.

[00:19:41] And then the second level is Trumpis actually willing to say thingsthat are true about American decline, right.

[00:19:47] So do you feel like you as an investor, as a venturecapitalist, got anything out of the first Trump term.

[00:19:56] Like what did Trump do in his first termthat you felt was anti decadent or anti stagnation?

[00:20:03] If anything, maybe the answer is nothing.Well I think we I think it took longer

[00:20:09] and it was slower than I would have liked.But we have, we have gotten to the place where a lot

[00:20:18] of people think something’s gone wrong.And that was not the conversation

[00:20:22] I was having in 2012, 2013, 2014.I had a debate with Eric Schmidt in 2012

[00:20:28] and Marc Andreessen in 2013 and Bezos in 2014.

[00:20:32] I was on the there’s a stagnation problem,and all three of them were versions of everything’s going

[00:20:38] great.And I think at least those three people have,to varying degrees, updated and adjusted.

[00:20:45] Silicon Valley is adjusted and Silicon Valley, though

[00:20:49] has more than adjusted.A a big part of Silicon Valley on the stagnation

[00:20:53] on the stagnation, stagnation.But then a big part of Silicon Valley

[00:20:58] ended up going in for Trump in 2024, including, obviously,

[00:21:04] most famously, Elon Musk.Yeah, this is deeply linked to the stagnation

[00:21:07] issue in my telling.I mean, these things are always super complicated,but my telling is I don’t.

[00:21:13] And again, I’m so hesitant to speak for all these people.But someone like Mark Zuckerberg or Facebook meta.

[00:21:21] And in some ways, I don’t think he was very ideological.

[00:21:25] He didn’t think this stuff through that much.It was the default was to be liberal.

[00:21:31] And it was always what if the liberalism isn’t working,

[00:21:35] what do you do.And for year after year after year,it was do more if something doesn’t work,

[00:21:39] you just need to do more of it.And you up the dose and you up the dose and youspend hundreds of millions of dollars

[00:21:44] and you go completely woke and everybody hates you.

[00:21:48] And at some point, it’s like, O.K, maybe this isn’t working.

[00:21:53] So they pivot.So it’s not a pro-trump thing.

[00:21:56] It’s not a pro-trump thing, but it is just both in public

[00:22:00] and private conversations.It is a kind of sense that Trumpism and populism in 2024,

[00:22:08] maybe not in 2016, when Peter was out thereas the lone supporter.

[00:22:14] But now in 2024, they can be a vehiclefor technological innovation, economic dynamism.

[00:22:22] So that’s your framing it really,really optimistically here.

[00:22:26] So I. Well the people but I think I know you’repessimistic.You frame this optimistically.

[00:22:32] You’re just saying these people are going to bedisappointed and they’re just set up for failure.

[00:22:37] And things like, I mean, peopleexpressed a lot of optimism.That’s all I’m saying.

[00:22:41] Elon Musk expressed a lot of I mean,he expressed some apocalyptic anxieties

[00:22:46] about how budget deficits were going to kill us all.But he came into government and people around him came

[00:22:51] into government basically saying,we have a partnership with the Trump administration,

[00:22:56] and we’re pursuing technological greatness.I think they were optimistic.

[00:23:00] And so I’m you’re coming from a place of greater pessimism

[00:23:05] or realism.So I’m just what I’m asking for is your assessment

[00:23:09] of where we are, not their assessment.But like, do you think does populism in Trump 2.0

[00:23:16] look like a vehicle for technological dynamism to you.

[00:23:21] It’s still by far the best option we have, I don’t think.

[00:23:27] I don’t know.Is Harvard going to cure dementia by just puttering

[00:23:33] along, doing the same thing that hasn’t worked for 50

[00:23:37] years.So that’s just a case for it.

[00:23:41] Can’t get it, can’t get worse.Let’s do disruption.But the critique of the critique of populism right

[00:23:47] now would be Silicon Valley made an alliancewith the populists.

[00:23:51] But in the end, the populists don’t care about science.They don’t want to spend money on science.

[00:23:56] They want to kill funding to Harvard just because theydon’t like Harvard.

[00:23:59] And in the end, you’re not going to get the kind

[00:24:03] of investments in the future that Silicon Valley wanted.

[00:24:07] Is that wrong.Yeah, but it.

[00:24:11] We have to go back to this question of, how well is this.

[00:24:16] Is the science working in the background.This is where the new dealers.

[00:24:20] Whatever was wrong with them.They pushed science hard and you funded it,

[00:24:25] and you gave money to people and you scaled it.And whereas today, if there was an equivalent of Einstein

[00:24:34] and he wrote a letter to the White House,it would get lost in the mail room,

[00:24:38] and the Manhattan Project is unthinkable.If we call something a moonshot the way this is

[00:24:44] the way Biden talked about, let’s say, cancer research,a moonshot in the 60s still meant that you went

[00:24:50] to the moon.A moonshot now means something completely fictional.That’s never going to happen.

[00:24:54] No, you need a moonshot for that.It’s not like we need an Apollo program.

[00:24:58] It means it’s never, ever going to happen.And so.But it seems like then you’re still in the mode of for you,

[00:25:05] as opposed to maybe for some other people in SiliconValley.

[00:25:09] The value of populism is in tearing away the veils

[00:25:13] and illusions, and we’re not necessarily in the stage whereyou’re looking to the Trump administration to build

[00:25:21] the new, to do the Manhattan Project, to do the moonshot.It’s more like populism helps us see that it was all fake.

[00:25:27] You need to try to do both.And they’re very entangled with each other.

[00:25:33] And I don’t know, there’s a deregulation of nuclear power.

[00:25:38] And at some point, at some point we’ll get backto building, new nuclear power plants or better designed

[00:25:46] ones, or maybe even fusion reactors.And, and so, yes, there’s a deregulatory,

[00:25:53] deconstructive part.And then at some point, you actually to get

[00:25:59] to construction and it’s all things like that.So yeah, in some ways, in some ways you’re clearing the field

[00:26:05] and then but you’ve maybe but you’ve personally stopped

[00:26:08] funding politicians I am schizophrenic on this stuff.

[00:26:13] I think it is.It is it’s incredibly important and it’s incredibly

[00:26:18] toxic.And so I go I go back and forth

[00:26:23] on incredibly toxic for you personally for everybody,

[00:26:29] everybody who gets involved.It’s 0 sum.It’s crazy.And then it’s and then in some ways because everyone hates

[00:26:36] you and associates you with Trump.Like how is it toxic for you personally.

[00:26:42] It’s toxic because it’s in a zero sum world.

[00:26:47] The stakes in it feel really, really high.

[00:26:51] And you end up having enemies you didn’t have before.

[00:26:55] It’s toxic for all the people who get involved in differentways.There is a political dimension of getting "Back to the Future."

[00:27:01] I don’t know.This is a conversation I had with Elon back in 2024.

[00:27:08] And we had all these conversations.I had the seasteading version with Elon where I said

[00:27:14] if Trump doesn’t win, I want to just leave the country.

[00:27:18] And then Elon said, there’s nowhere to go.There’s nowhere to go.

[00:27:22] This is the only place.And then you always think of the right argumentsto make later.

[00:27:26] And it was about two hours after we had dinner and I washome that I thought of, wow, Elon,

[00:27:31] you don’t believe in going to Mars anymore.2024 2024 is the year where Elon

[00:27:38] stopped believing in Mars.Not as a silly science tech project, but as

[00:27:44] a political project.Mars was supposed to be a political project.It was building an alternative.

[00:27:49] And in 2024, Elon came to believethat if you went to Mars the socialist US

[00:27:55] government, the woke A.I., it would follow you to Mars.

[00:27:59] It was the dumbest meeting with Elon that we brokered.

[00:28:03] He was doing DeepMind.This is an A.I. company.Yeah this was the rough conversation was Dennis tells

[00:28:08] Ellen, I’m working on the most important projectin the world.I’m building a superhuman A.I.

[00:28:13] And Ellen responds to Dennis, well,I’m working on the most important projectin the world.

[00:28:17] I am turning us into an interplanetary species.And then Dennis said, well, my A.I.

[00:28:24] will be able to follow you to Mars and.And then Ellen went quiet.

[00:28:28] But in my telling of the history,it took years for that to really hit Ellen.

[00:28:34] It took him until 2024 to process it.But that doesn’t mean he doesn’t believe in Mars.

[00:28:39] It just means that he decided hehad to win some kind of battle over budget deficits

[00:28:44] or wokeness to get to Mars.What does Mars mean.Is it a Yeah.

[00:28:48] Is it.And again, it’s what does Mars mean.Well, it was is it just is it just a scientific project

[00:28:57] or is it I don’t know.Is it like A.I. don’t know, high vision of a new society.

[00:29:02] Yeah Heinlein.Populated by many, many libertarian paradise

[00:29:07] or something like Elon Musk.Well, I assume it was concretized that specifically.

[00:29:12] But if you concretize things, then maybe you realize that

[00:29:18] Mars is supposed to be more than a science project,it’s supposed to be a political project.

[00:29:22] And then when you concretize it,you have to start thinking through,well, the I woke, I will follow you,

[00:29:27] the socialist government will follow you,and then maybe you have to do something other

[00:29:31] than just going to Mars.O.K., so the woke A.I.

[00:29:36] Artificial intelligence seems like one.

[00:29:40] If we’re still stagnant, it’s the biggest exceptionto stagnation.

[00:29:44] Yes it’s the place where there’s been.Yes remarkable progress.

[00:29:48] Surprising to many people.Progress it’s also the place we were just talking about

[00:29:51] politics.It’s the place where the Trump administration is, I think,

[00:29:56] to a large degree, giving A.I. investors a lot of what they

[00:30:00] wanted in terms of both stepping back and doing public

[00:30:05] private partnerships.So it’s a zone of progress and governmental engagement.

[00:30:12] And you are an investor in A.I.What do you think you’re investing in?

[00:30:20] Well, I don’t know, there’s a lot of layers to this.

[00:30:31] So I do think I know there’s one,

[00:30:36] one question we can frame is just how big how big a thing

[00:30:40] do I think AI is.And I don’t my stupid answer is it’s somewhere it’s more

[00:30:48] than a nothing burger and it’s less than the totaltransformation of our society.

[00:30:53] So my placeholder is that it’s roughly on the scaleof the internet in the late 90s,

[00:30:58] which is I’m not sure it’s enough to really endthe stagnation.

[00:31:03] It might be enough to create some great companies.And the internet added maybe a few points, percentage points

[00:31:12] to the GDP, maybe 1 percent to GDP growth every year

[00:31:16] for 10, 15 years.It added some to productivity.And so that’s of roughly my placeholder for I.

[00:31:22] It’s the only thing we have.It’s a little bit unhealthy that it’s so unbalanced.

[00:31:26] This is the only thing we have.I’d like to have more multi-dimensional progress.I’d like us to be going to Mars.

[00:31:31] I’d like us to be having cures for dementia.If all we have is I will take it.

[00:31:37] There are risks with it.There are obviously there are dangers with this technology.

[00:31:44] So you’re a skeptic.But then you are a skeptic of what you might call

[00:31:51] the superintelligence cascade theory,which basically says that if AI succeeds,

[00:31:57] it gets so smart that it gives us the progress in the world

[00:32:02] of atoms, that it’s like, all right, we can’t cure dementia.

[00:32:06] We can’t figure out how to build the perfect factory that

[00:32:12] builds the rockets that go to Mars.But I can and and at a certain point,

[00:32:16] you pass a certain threshold and it gives usnot just more digital progress, but 64 other forms

[00:32:23] of progress.It sounds like you don’t believe that,or you think that’s less likely.

[00:32:28] Yeah I somehow don’t know if that’s been really the gating

[00:32:36] factor.What does that mean.The gating factor.It’s probably a Silicon Valley ideology.

[00:32:42] And maybe in a weird way it’s more liberal thana conservative thing.

[00:32:46] But people are really fixated on IQ in Silicon Valley

[00:32:51] and that it’s all about smart people.And if you have more smart people, they will.

[00:32:58] Do great things.And then the economics antique argument

[00:33:05] is that people actually do worse.The smarter they are, the worse they do.

[00:33:10] And, it’s just they don’t know how to apply it or our society

[00:33:14] doesn’t know what to do with them and they don’t fit in.And so that suggests that the gating factor isn’t IQ,

[00:33:20] but something that’s deeply wrong with our society.So is that a limit on intelligence

[00:33:26] or a problem of the personality types.

[00:33:29] Well, it’s human superintelligence creates.I mean, I’m very sympathetic to the idea.

[00:33:34] And I made this case when I did an episode of this podcast

[00:33:39] with AI accelerationist that just throwing

[00:33:43] that certain problems can just be solvedif you ramp up intelligence.

[00:33:47] It’s like we ramp up intelligence and boom,Alzheimer’s is solved.We ramp up intelligence and the I can,

[00:33:53] figure out the automation process that builds youa billion robots overnight.

[00:33:57] I’m an intelligent skeptic in the sense I don’t think.

[00:34:01] Yeah, I think you probably have limits.It’s hard to prove one way.

[00:34:06] It’s always hard to prove these things.But I until we have the superintelligence,

[00:34:10] I share your intuition because I think we’ve had a lot

[00:34:14] of smart people and things have been stuck for other

[00:34:18] reasons.And so maybe the problems are unsolvable,

[00:34:22] which is the pessimistic view.Maybe there is no cure for dementia at all.And it’s a deeply unsolvable problem.

[00:34:27] There’s no cure for mortality.Maybe it’s an unsolvable problem,

[00:34:32] or maybe it’s these cultural things.So it’s not the individually smart person,

[00:34:38] but it’s how this fits into our society.Do we tolerate heterodox smart people.

[00:34:44] Maybe it’s maybe you need heterodox smart peopleto crazy experiments and if the AI is just conventionally

[00:34:56] smart, if we define wokeness, again,

[00:35:01] wokeness is too ideological, but if you just define itas conformist, maybe that’s not the kind of smartness

[00:35:07] that’s going to make a difference.So do you fear, then, a plausible future where AI,

[00:35:15] in a way becomes itself stagnation,that it’s like highly intelligent,

[00:35:19] creative in a conformist way.It’s like the Netflix algorithm.

[00:35:24] It makes infinite O.K movies that people watch.

[00:35:28] It generates infinite O.K IDs.It puts a bunch of people out of work

[00:35:32] and makes them obsolete.But it doesn’t.It like deepens stagnation in some way.

[00:35:37] Is that a fear.It’s like people just outsource.

[00:35:45] It’s quite possible that that’s certainly a risk.But I guess where I end up is I still

[00:35:54] think we should be trying.And that the alternative is just total stagnation.

[00:36:00] So yeah, there’s all sorts of interesting things goingto happen with maybe drones in a military context are

[00:36:06] combined with AI and O.K, this is kind of scary or dangerous

[00:36:10] or dystopian or it’s going to change things.But if you don’t have AI, Wow, there’s just nothing going on.

[00:36:19] And I don’t this is there’s a version of this discussion

[00:36:22] on the internet where did the internet lead to more

[00:36:27] conformity and more wokeness.And yeah, there are all sorts of ways where it didn’t lead

[00:36:33] to quite the cornucopian diverse explosion of ideas

[00:36:37] that libertarians fantasized about in 1999.

[00:36:42] But counterfactually, I would argue that it was still better

[00:36:45] than the alternative, that if we hadn’t had the internet,

[00:36:50] maybe it would have been worse.I bet it’s better than the alternative.

[00:36:54] And the alternative is nothing at all.Because the.Look here’s one place where the stagnation arguments are

[00:37:00] still reinforced.The fact that we’re only talking about I feel,

[00:37:05] is always an implicit acknowledgment that.But for we are like in almost total stagnation.

[00:37:14] But the world of A.I. is clearly filled with people who

[00:37:19] at the very least seem to have a more utopian,

[00:37:25] transformative, whatever word you want to call it viewof the technology than you’re expressing here.

[00:37:30] And you were mentioned earlier,the idea that the modern world used to promise radical life

[00:37:36] extension and doesn’t anymore.It seems very clear to me that a number of people deeply

[00:37:41] involved in artificial intelligencesee it as a kind of mechanism for transhumanism,

[00:37:46] for transcendence of our mortal flesh,and either some kind of creation of a successor

[00:37:53] species or some kind of merger of mind and machine.

[00:37:57] And do you think that’s just all kind of irrelevant

[00:38:02] fantasy, or do you think it’s just hype.

[00:38:06] Do you think people are trying to raise money by pretendingthat we’re going to build a machine.

[00:38:12] God right.Is it hype.Is it delusion.Is it something you worry about.

[00:38:17] I think you would.You would prefer the human race to endure.You’re hesitating.

[00:38:23] Well, I Yes, I would.

[00:38:29] This is a long hesitation.There’s a long hesitation.There’s so many questions.

[00:38:33] And should the human race survive.

[00:38:38] Yes O.K. But I also would.

[00:38:44] I also would like us to radically solve

[00:38:47] these problems.And so it’s always I don’t know.

[00:38:54] Yeah transhumanism is this the ideal was

[00:38:59] this radical transformation whereyour human natural body gets transformed

[00:39:05] into an immortal body.And there’s a critique of let’s say,

[00:39:11] the trans people in the sexual context or I don’ttransvestite is someone who changes their clothes

[00:39:18] and cross-dresses, and a transsexual is someone where

[00:39:21] you change your I don’t penis into a vagina.And we can then debate how well those surgeries work,

[00:39:27] but we want more transformation than that.It’s the critique is not that it’s weird and unnatural.

[00:39:33] Man, it’s so pathetically little.And we want more than cross-dressing or changing

[00:39:39] your sex organs.We want you to be able to change your heartand change your mind and change

[00:39:44] your whole body and then orthodox Christianity.

[00:39:48] By the way, the critique orthodox Christianity hasof these things don’t go far enough like that.

[00:39:55] Transhumanism is just changing your body.But you also need to transform your soul,

[00:39:59] and you need to transform your whole self.And so.

[00:40:03] But the other one.Wait wait wait, sorry, I generally agree with your

[00:40:09] what I think is your belief that religion should

[00:40:14] be a friend to science and ideas of scientific progress.I think any idea of divine Providence

[00:40:20] has to encompass the fact that we have progressedand achieved and done things that

[00:40:25] would have been unimaginable to our ancestors.But it still also seems like, Yeah,

[00:40:32] the promise of Christianity in the end is you getthe perfected body and the perfected soul through God’s

[00:40:39] grace.And the person who tries to do it on their ownwith a bunch of machines is likely to end up

[00:40:45] as a dystopian character.Well, it’s.

[00:40:52] Let’s articulate this.And you can have a heretical form of Christianity.

[00:40:57] That says something else.I don’t know.

[00:41:01] I think the word nature does not occur oncein the Old Testament.

[00:41:05] And so if you and there is a word

[00:41:12] in which a sense in which the way I understand,

[00:41:17] the judeo-christian inspiration is it

[00:41:22] is about transcending nature.It is about overcoming things.And the closest thing you can say to nature is that people

[00:41:30] are fallen and that that’s the natural thing in a Christian

[00:41:34] sense, is that you’re messed up.And that’s true.

[00:41:39] But there’s some ways that with God’s help are supposed

[00:41:46] to transcend that and overcome that.But the people, if you just present say you’re accepted,

[00:41:52] present company accepted.Most of the people working to build

[00:41:56] the hypothetical machine.God don’t think that they’re cooperating with Yahweh,

[00:42:02] Jehovah, the Lord of hosts.They think they think that they’re building immortality

[00:42:07] on their own.Yeah, right.We’re jumping around a lot.A lot of things.So again the critique I was saying is they’re not

[00:42:15] ambitious enough.From a Christian point of view,these people are not ambitious enough.

[00:42:18] Now then we get into this question.Well, are they not.But they’re not morally and spiritually ambitious enough.

[00:42:25] And are they.And then are they are they still

[00:42:28] physically ambitious enough.And are they are they even still really transhumanists?

[00:42:34] And this is where O.K. Man, the cryonics thing that seems

[00:42:40] like a retro thing from 1999.There isn’t that much of that going on.

[00:42:43] So they’re not transhumanists on a physical body.And then, O.K, well, maybe it’s not about cryonics,

[00:42:48] maybe it’s about uploading.O.K, well, it’s not quite.I’d rather have my body.

[00:42:52] I don’t want just a computer program that simulates me.So that uploading seems like a step down from cryonics, but.

[00:42:59] But then even that’s it’s part of the conversation.

[00:43:05] And this is where it gets very hard to score.And I don’t want to say they’re all making it up

[00:43:09] and it’s all fake, but I don’t think you think some of it’s

[00:43:13] fake.I don’t think it’s fake implies people are lying.

[00:43:18] But I want to say it’s not the center of gravity.Yeah and so there is.

[00:43:23] Yeah, there is a cornucopian language.There’s an optimistic language.

[00:43:28] A conversation I had with Elon a few weeks ago about thiswas, he said, we’re going to have a billion humanoid robots

[00:43:38] in the US in 10 years.And I said, well, if that’s true,

[00:43:41] you don’t need to worry about the budget deficitsbecause we’re going to have so much growth.

[00:43:46] The growth will take care of this.And then.Well, he’s still worried about the budget deficits.

[00:43:50] And then this doesn’t prove that he doesn’t believein the billion robots.

[00:43:54] But it suggests that maybe he hasn’t thought it through

[00:43:58] or that he doesn’t think it’s going to be as transformativeeconomically or that there are big error bars around it.

[00:44:05] But yeah, there’s some way in which these things are notquite thought through.

[00:44:10] If I had to give a critique of Silicon Valley,it’s always bad at what the meaning of tech is

[00:44:17] and the conversations, they tend to go into this

[00:44:21] microscopic thing where it’s O.K, it’s like,what are the IQ, Helo scores of the AI.

[00:44:28] And exactly how do you define AGI.And we get into all these endless technical debates.

[00:44:34] And there are a lot of questionsthat are at an intermediate level of meaning

[00:44:40] that seem to me to be very important, which is like,what does it mean for the budget deficit.

[00:44:44] What does it mean for the economy.What does it mean for geopolitics.

[00:44:48] One of the conversations, we had recently was

[00:44:53] and I had was, does it change the calculus

[00:44:58] for China invading Taiwan, wherewe have an accelerating AI revolution in the military.

[00:45:05] Is China falling behind.And will this and maybe on the optimistic side,

[00:45:11] it deters China because they’ve effectively lost.

[00:45:15] And on the pessimistic side it accelerates them because theyknow it’s now or never.

[00:45:19] If they don’t grab Taiwan now they will fall behind.

[00:45:23] And either way, this is a pretty important thing.It’s not thought through.

[00:45:27] We don’t think about what AI means for geopolitics.We don’t think about what it means for the macro economy.

[00:45:34] And those are the kinds of questions I’d want us to pushmore.There’s also a very macroscopic question that

[00:45:41] you’re interested in that, will pull on the religion

[00:45:45] thread a little bit here.You have been giving talks recently

[00:45:50] about the concept of the Antichrist, which

[00:45:54] is a Christian concept, an apocalyptic concept.What does that mean to you.

[00:45:59] What is the antichrist?How much time do we have.We’ve got as long.

[00:46:03] As much time as you have to talk about the Antichrist.

[00:46:07] All right, well, I have A.I. could talk about it,but we’re near time.I mean, but no, I think there’s always a question,

[00:46:18] how do we articulate some of these existential risks,

[00:46:24] some of the challenges we have.And they’re all framed in this runaway dystopian science

[00:46:30] text.There’s a risk of nuclear war.There’s a risk of environmental disaster.

[00:46:35] Maybe something specific like climate change.Although there are lots of other oneswe could come up with.

[00:46:39] There’s a risk of I don’t know, bioweapons.

[00:46:43] You have all the different sci-fi scenarios.Obviously, there are certain types of risks with A.I.

[00:46:48] But I always think that if we’re going to have this frameof talking about existential risks,

[00:46:54] perhaps we should also talk about the risk of another type

[00:46:58] of a bad singularity, which I would describe as the oneworld totalitarian state because I would say

[00:47:05] the political solution, the default political solution

[00:47:11] people have for all these existential risks is one world

[00:47:17] governance.What do you do about nuclear weapons.We have a United Nations with real teeth that controls them.

[00:47:26] And it’s they’re controlled by an international political

[00:47:30] order.And then something like this is also,

[00:47:37] what do we do about A.I. and we needglobal compute governance.

[00:47:41] We need a one world government to control all the computers,

[00:47:46] log every single keystroke to make sure people don’t program

[00:47:50] a dangerous A.I.And I’ve been wondering whether that’s going from

[00:47:55] the frying pan into the fire.And so the atheist philosophical framing

[00:48:01] is one world or none.That was a short film that was put out by the Federation

[00:48:06] of American Scientists in the late 40s,starts with a nuclear bomb blowing up the world.

[00:48:11] And obviously you need a one world governmentto stop it, one world or none.

[00:48:16] And the Christian framing, which in some ways

[00:48:20] is the same question, is Antichrist or armageddon?

[00:48:24] You have the one world state of the Antichrist,or we’re sleepwalking towards Armageddon.

[00:48:30] One world or none.Anti-christ or Armageddon.On one level are the same.

[00:48:35] Question now, I have a lot of thoughts on this topic,

[00:48:40] but one question is and this was

[00:48:44] a plot hole in all these Antichrist bookspeople wrote, how does the Antichrist take over

[00:48:50] the world.He gives these demonic, hypnotic speeches and people

[00:48:55] just fall for it.And so it’s this plot hole.It’s this demonic.

[00:48:59] It’s totally it’s implausible.It’s a very implausible plot hole.But I think we have an answer to this plot hole.

[00:49:05] The way the Antichrist would take over the worldis you talk about Armageddon non-stop,

[00:49:11] you talk about existential risk non-stop.And this is what you need to regulate.

[00:49:16] It’s the opposite of the picture of baconian sciencefrom the 17, 18th century, where the Antichrist is like

[00:49:24] some evil tech genius, evil scientist who invents this

[00:49:29] machine to take over the world.People are way too scared for that.

[00:49:33] In our world, the thing that has political resonanceis the opposite.

[00:49:37] It is.It is.The thing that has political resonanceis we need to stop science.

[00:49:42] We need to just say stop to this.And this is where Yeah, I don’t know.

[00:49:47] In the 17th century, I can imagine a Doctor Strangelove

[00:49:51] Edward Teller type person taking over the world.

[00:49:55] In our world, it’s far more likely to be Greta Thunberg.

[00:49:59] O.K, I want to suggest a middle groundbetween those two options.

[00:50:02] It used to be that the reasonable fearof the Antichrist was a kind of Wizard of technology,

[00:50:09] and now the reasonable fear is someonewho promises to control technology, make it safe,

[00:50:15] and Usher in what, from your point of view,would be a kind of universal stagnation.

[00:50:20] Well, it’s more that’s more my description of how it would

[00:50:24] happen.So I think people still have a fear of a 17th century

[00:50:29] anti-christ.We’re still scared of Doctor Strangelove, right.But you’re saying you’re saying the real Antichrist

[00:50:34] would play on that fear and say,you must come with me to avoid Skynet,

[00:50:39] to avoid the Terminator, to avoid nuclear armageddon?Yes And I guess my view would be looking at the world

[00:50:45] right now, that you would need a certain kind

[00:50:49] of novel technological progressto make that fear concrete.

[00:50:56] So I can buy that the world could

[00:50:59] turn to someone who promised peace and regulation.

[00:51:03] If the world became convinced that I was

[00:51:07] about to destroy everybody.But I think to get to that point,

[00:51:11] you need one of the accelerationist apocalyptic

[00:51:14] scenarios to start to play out to get your peace and safety

[00:51:19] anti-christ, you need more technological progress.Like one of the key failures of totalitarianism in the 20th

[00:51:25] century was it had a problem of knowledge.It couldn’t know what was going on.

[00:51:30] All over in the world.So you need the A.I. or whatever else

[00:51:34] to be capable of helping the peace and safety totalitarian

[00:51:38] rule.So don’t you think you need essentially need your worst

[00:51:45] case scenario to involve some burst of progress that is then

[00:51:51] tamed and used to impose stagnant totalitarianism.

[00:51:56] You can’t just get there from where we are right now.Well, it can Greta Thunberg’s on a boat

[00:52:02] in the Mediterranean.Protesting Israel the.

[00:52:07] I just don’t see the promise of safety from A.I.,

[00:52:12] safety from tech safety, even safety from climate changeright now as a powerful universal rallying cry.

[00:52:20] Absent accelerating change and real fearof total catastrophe.

[00:52:26] I mean, these things are so hard to score,but I think environmentalism is pretty powerful.

[00:52:34] I don’t know if it’s I don’t know if it’s absolutely

[00:52:37] powerful enough to create a one world totalitarian state.But man, it is.

[00:52:42] I think it is not.It is in its current form.It is.I want to say it’s the only thing people still believe

[00:52:47] in Europe.They believe in the green thingmore than Islamic Sharia law or more

[00:52:54] than in the Chinese Communist totalitarian takeover.

[00:52:59] And the future is an idea of a future that looksdifferent from the present.

[00:53:02] The only three on offer in Europeare green, Sharia and the totalitarian Communist state.

[00:53:11] And the green one is by far the strongest

[00:53:15] and in a declining, decaying Europe.But it’s not a dominant player in the world.

[00:53:20] It’s always in a context.And then, I don’t we had this really complicated history

[00:53:27] with the way nuclear technology worked.And we O.K. We didn’t Yeah.

[00:53:33] We didn’t really get to a totalitarian one world state.

[00:53:37] But by the 1970s, one account of the stagnation is that

[00:53:42] the runaway progress of technology had gotten very

[00:53:46] scary and that baconian science, it ended at Alamos.

[00:53:51] And then it was O.K. It ended there.And we didn’t want to have any more.

[00:53:56] And, when Charles Manson took LSD in the late 60s

[00:54:03] and started murdering people, what he saw on LSD,what he learned was that you could be like Dostoevsky,

[00:54:10] an anti-hero in Dostoevsky, and everything was permitted.And of course, not everyone became Charles Manson,

[00:54:15] but Charles Manson.But crucially of the history, everyone became as deranged

[00:54:20] as Charles Manson.But Charles Manson did not become the Antichrist

[00:54:24] and take over the world.I’m just.I’m just.We’re ending.We’re ending in the apocalyptic.

[00:54:28] No, but you’re my telling of the.My telling of the history of the 1970s is the hippies did

[00:54:34] win and they.But we landed.We landed on the moon in July of 1969.

[00:54:41] Woodstock started three weeks later.And with the benefit of hindsight,

[00:54:45] that’s when progress stopped and the hippies won.And yeah, it was not literally Charles Manson.

[00:54:49] But you’re just I want to stay with the Antichrist justto end.

[00:54:53] Because and you’re retreating, you’re saying,O.K environmentalism is already pro stagnation

[00:55:00] and so on.O.K, let’s agree with all that.I’m just saying we’re not living under we’re not living

[00:55:06] under the Antichrist right now.We’re just stagnant.And you’re positing that something worse could be

[00:55:12] on the horizon.That would make stagnation permanent.That would be driven by fear.

[00:55:17] And I’m suggesting that for that to happen,

[00:55:21] there would have to be some burst of technologicalprogress that was akin to Alamos that people are afraid

[00:55:28] of.And I guess this is my very specific question for you,

[00:55:33] right.Is that, well, you are you’re an investor in A.I.

[00:55:37] You’re deeply invested in Palantir,

[00:55:40] in military technology and technologies,of surveillance and technologies of warfare

[00:55:46] and so on.And it just seems to me that when you tell me

[00:55:50] a story about the Antichrist coming to power

[00:55:54] and using the fear of technological changeto impose order on the world, I

[00:56:01] feel like that Antichrist would maybebe using the tools that you were, that you were building.

[00:56:07] Wouldn’t the Antichrist be like,great we’re not going to have any more technological

[00:56:12] progress.But I really like what Palantirhas done so far right.

[00:56:16] I mean, isn’t that a concern.Wouldn’t that be the irony of history would be that the man

[00:56:22] publicly worrying about the Antichrist accidentally

[00:56:26] hastens his or her arrival.

[00:56:31] There look, there are all these different scenarios.

[00:56:35] I obviously don’t think that that’s what I’m doing.I mean, to be clear, I don’t think that’s I don’t think

[00:56:41] that’s what you’re doing either.I’m just interested in how you get to a world willing

[00:56:46] to submit to permanent authoritarian rule.

[00:56:49] Well, but again, there are these different gradations

[00:56:56] of this we can describe.But is this so preposterous, what I’ve just told you,

[00:57:04] as a broad account of the stagnation that the entire

[00:57:08] world has submitted for 50 years to peace and safety.

[00:57:14] This is a first Thessalonians 5’ 3.The slogan of the anti-christ is peace and safety.

[00:57:19] And we’ve submitted to.The FDA regulates not just drugs in the US, but facto

[00:57:28] in the whole world.Because the rest of the world defers to the FDA.The Nuclear Regulatory commission

[00:57:33] effectively regulates nuclear power plantsall over the world.

[00:57:36] People you can’t design a modular nuclear reactor

[00:57:40] and just build it in Argentina.They won’t trust the Argentinian regulators.

[00:57:46] They’re going to defer to the US.And so it is at least it’s at least a question about why

[00:57:52] we’ve had 50 years of stagnation.And one answer is we ran out of ideas.

[00:57:57] The other answer is that something happened culturallywhere it wasn’t allowed.

[00:58:02] And then the cultural answer can be a bottom up answer,that it was just some transformation of humanity

[00:58:09] into the more docile kind of a species,

[00:58:14] or it can be at least partially top down

[00:58:18] that there is this machinery of government that got changed

[00:58:24] into this stagnation thing.I think something like this nuclear power was supposed

[00:58:29] to be the power of the 21st century.And it somehow has gotten off, ramped

[00:58:36] all over the world on a worldwide basis.So in a sense, we’re already living under a moderate rule

[00:58:43] of the Antichrist in that telling,what do you think God is in control of history.

[00:58:50] I mean, this is again A.I. think there’s always room for human

[00:59:02] freedom and human choice.These things are or at least where we are today.

[00:59:10] These things are they’re not absolutely predetermined one

[00:59:18] way or another.But God wouldn’t leave us forever under the rule

[00:59:24] of a mild, moderate Antichrist, right.

[00:59:29] That can’t be how the story ends, right.It’s attributing too much causation to God is always

[00:59:38] a problem.I know there are different Bible verses I can give you,but I’ll give you John 1525 where Christ says,

[00:59:46] they hated me without cause and so as all these people

[00:59:51] that are persecuting Christ have no reason,

[00:59:55] no cause for why they’re persecuting Christ,and if we interpret this as a ultimate causation verse,

[01:00:03] they want to say, I’m persecuting because God causedme to do this.God is causing everything.

[01:00:09] And the Christian view is anti-calvinist.God is not behind history.

[01:00:14] God is not causing everything.If you say God is causing everything, but God is.

[01:00:18] But wait.But God is.You’re scapegoating God, but God is your scapegoat.But God is behind Jesus Christ entering history

[01:00:25] because God was not going to leave usin a stagnation as decadent Roman Empire, right.

[01:00:31] Well, so at some point, at some point,no, no, at some point God is going to step in.

[01:00:36] I am not, I am not, I am not that Calvinist.And that’s not Calvinism, though.

[01:00:41] That’s just Christianity.God God will not leave us eternally staring into screens

[01:00:48] and being lectured by Greta Thunberg, right.He will not abandon us to that fate.

[01:00:54] It is.It is.There is a great, I don’t know,

[01:01:00] for better and for worse.I think there’s a great deal of scope for human action,

[01:01:06] for human freedom.If I thought these things were deterministic,

[01:01:13] you might as well maybe just accept it.The lines are coming.

[01:01:17] You should just have some yoga and prayerful meditationand wait while the lines eat you up.

[01:01:22] And I don’t think that’s what you’re supposed to do.It’s no, I agree with that.

[01:01:27] And I think on that note, I’m just trying to be hopeful

[01:01:30] and suggesting that in trying to resist the Antichrist using

[01:01:35] your human freedom, you should have hope that you’ll succeed.We can agree on that.

[01:01:41] Good Peter Thiel, thank you for joining me.Thank you.


